Error Back Propagation Algorithm

Gradient Descent : 함수값이 낮아지는 방향으로 학습을 하는 것으로 무작위 점을 찍고 그 점을 기준으로 함수값이 낮아지는 방향을 판단하고 함수값을 조금씩 해당 방향으로 옮기다 기울기가 0에 가깝게 되면 해당 지점이 최저값이라고 판단을 함. 하지만 1차 적으로만 판단할 수 있음.(로컬 미니멈만 탐색 가능) 손실함수에 따라서 포인트를 찍기 때문에 기울기가 경사질수록 포인트의 개수가 적어지는 특징이 있음.

하이퍼 파라미터 : 우리가 정해둘 수 있는 파라미터를 말함.

Gradient Descent에서 step size가 너무 크게 되어버리면 손실이 생길 수 있음. step size를 적당하게 크게 설정하면 그래프에 있는 마루를 넘을 수 있는 힘을 주어 글로벌 미니멈을 구할 수 있다. step size가 너무 작으면 기울기가 가팔라도 너무 탐색을 많이 하게 되어서 자원적 손실이 발생한다. 적절한 step size는 노가다를 통해서 찾아야 한다. 하지만 찾기가 힘들기 때문에 로컬 미니멈에 갇히는 점이 Gradient Descent의 한계이다. 

Error Back Propagation : 가중치를 스스로 판단하여 최적화 시키는 것. Feed Forward란 입력을 넣고 출력을 나오게 하는 과정을 말함. 이것을 하는 이유는 에러를 기반으로 입력해주는 가중치를 업데이트하는 과정이기 때문임. 에러라는 것은 두 값을 비교해서 얼마가 차이가 나는지 계산하는 것이고, ai가 순전파를 계산하여 오차값을 구할 수 있음. 에러를 구하는 바법은 (target – output)제곱을 2로 나눈 값이다. 나누는 수는 더해준 개수만큼 나눠주는 것이다. 출력에서 입력으로 가면서 값을 업데이트 해주는 것을 Back-Propagation이라고 함. 
Back-Propagation은 크게 2가지 파트로 볼 수 있는데 출력층에서랑 그 이전 층에서의 Back-Propagation이 있고, 이전 층과 이전 층의 이전 층은 앞에서 반영한 에러값을 모두 반영하여 계산을 따로 해줘야 한다. 그래서 앞으로 나아갈수록 값이 커진다. 그래서 이 과정을 많이 반복할수록 에러값이 줄어들게 된다. (미니멈을 점점 낮춰간다)

실습에서의 용어
download : 파일을 인터넷에서 가져와야 하기 때문에 다운로드를 해준다. download = True
shuffle : 내림차순이나 오름차순으로 되어 있으면 성능이 안좋아 질 수 있기 때문에 섞어준다.
g.manual_seed(42) : 시드를 고정하고 비교를 해줘야 학습이 의미가 있기 때문에 고정해 줘야 한다.
Flatten : 이미지를 펴준다. 28x28 이면 784로 펴준다.
Linear : 왼쪽 값을 오른쪽 값으로 압축해 준다.
ReLU : 비선형 문제 처리를 위한 활성화 함수
CrossEntropyLoss : 중간 값에 0이 나오는 것을 방지하기 위해 사용한다.
optimizer : 경사 하강법, 파라미터를 정해줘야 한다.
epoch : 학습을 하는 횟수
zero_grad : 전에 있던 가중치 학습 결과를 초기화 시켜준다.
with torch.no_grad() : 내가 아래에 한 모든 결과가 학습 결과에 영향을 주지 말라는 의미. 