# Error Back Propagation Algorithm

## Gradiend Descent
y = x^2 함수가 있을 때 최솟값을 구하는 것 

함수값이 낮아지는 방향으로 독립 변수들을 변형시킴 

랜덤한 점을 찍고 그 점의 기울기를 구함 

-> 음수이면 x 값을 오른쪽으로, 양수이면 왼쪽으로 이동하며 최솟값을 찾음 

1차적으로 최솟값을 찾기위해 최적화됨. 

-> y = x^2 함수의 경우 굴곡이 하나만 존재하여 찾기 쉽지만 굴곡이 여러개이고, 그래프가 엄청 긴 경우(자연적인 함수를 그래프화 했을 경우)에는 찾는게 정확하지 않을 수 있음 (한 구간에만 머물기 때문) 

에타(학습률)이 크면 빠르게 찾을 수 있음, 하지만 step size가 너무 커서 최솟값을 찾는데 어려움을 겪을 수 있다. 
-> 작은 언덕을 넘을 수 있는 원동력이 될 수 있음. 

에타(학습률)이 너무 작으면 global minimum이 아닌 곳을 넘어갈 수도 없을 뿐더러 그 local minimum 값을 찾는 것에도 너무 많은 자원이 쓰인다. 

적절한 에타를 설정해서 global minimum을 찾을 수도 있겠지만, 이를 설정하기 까다로울 뿐더러, initial point를 어디에 설정하냐에 따라 많은 오차가 있을 수 있음. 

## Error Back Propagation
딥러닝의 경우 가중치 또한 본인이 알아서 가중치를 알맞게 설정해준다.
그 과정중 하나가 Error Back Propagation

## Feed Forward (순전파)
입력들을 넣고, 출력이 나올 때까지의 과정

Error Back Propagation은 오차값(에러)이 있어야 하는데, 이를 구하기 위해 순전파를 진행한다.

(Error Back Propagation은 구해야하는 값에서 output값을 뺀 절댓값을 
에러로 설정해 거꾸로 가중치를 다시 설정함)

## Error
E = sigma((1/2)(target - output)^2) 

## Back Propagation
출력층에서 입력층 방향으로 가중치를 업데이트 하는 것

case 1. 출력층과 바로 이전층은 에러를 하나만 반영하면 되지만, 

case 2. 출력층 이전층과 그 이전층들은 뒤의 모든 에러를 반영해야한다. (에러값이 점점 커짐)

## 실습
torch.nn -> 파이썬에서 신경망을 구축할 수 있게 하는 라이브러리 

CrossEntropyLoss : 손실함수에서 출력값이 0이 되는 것을 막기위한 함수 

optimizer -> 최적화된 파라미터를 찾아준다. 

