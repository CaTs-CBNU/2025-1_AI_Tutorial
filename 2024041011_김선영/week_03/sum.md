Gradient Descent & Error Back Propagation 정리

Gradient Descent (경사 하강법)

- 정의: 함수의 기울기를 이용해 손실 함수(Loss Function)의 값을 최소화하는 최적화 알고리즘
- 작동 방식:
  - 임의의 점(초기값)에서 시작하여, 함수의 기울기를 계산
  - 기울기가 음수일 경우 → 오른쪽(x 증가)
  - 기울기가 양수일 경우 → 왼쪽(x 감소)
  - 이 과정을 반복하여 최솟값을 찾아감

- 한계점:
  - 전체 최솟값이 아닌 국소 최소값에 수렴할 수 있음
  - 함수의 구조나 초기값에 따라 결과가 달라짐

- 학습률 (Learning Rate, η):
  - 학습률은 사용자가 조정 가능한 하이퍼파라미터
  - 너무 크면 발산하거나 최솟값을 지나칠 수 있음
  - 너무 작으면 느리고, 자원이 많이 소모되며 local minimum에 갇힐 수 있음

- 예시 수식:  
  W = W - η * ∇L

- 적절한 학습률 선택이 중요  
- 예시 함수: `y = x²`  
  → 굴곡이 하나뿐이라 최솟값 찾기 쉬움  
  → 자연계 함수처럼 복잡한 형태일 경우, 어려움 존재

---

Error Back Propagation (오차 역전파)

- 정의: 신경망의 출력층에서 발생한 오차를 기반으로, 각 층의 가중치를 수정하는 알고리즘
- 딥러닝과 머신러닝을 구분짓는 핵심 요소

학습 흐름

1. Feed Forward (순전파) 
   - 입력층 → 은닉층 → 출력층으로 계산 진행

2. Error 계산
   - 실제 정답과 출력값의 오차 계산  
   - 예시 손실 함수:  
     E = Σ(1/2 × (target - output)²) 
     또는 `CrossEntropyLoss` 사용

3. Back Propagation (역전파)  
   - 출력층부터 입력층 방향으로 가중치를 수정  
   - 출력층과 가까운 층은 한 에러만 반영  
   - 입력층에 가까운 층은 여러 에러의 영향을 종합해야 함

---


